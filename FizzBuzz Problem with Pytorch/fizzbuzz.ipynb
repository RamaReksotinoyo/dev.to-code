{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0031f31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33e80cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_encode(i: int, num_digits: int) -> List[int]:\n",
    "    return np.array([i >> d & 1 for d in range(num_digits)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "96713419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0 0 0 0]\n",
      "[0 1 0 0 0 0 0 0 0 0]\n",
      "[1 1 0 0 0 0 0 0 0 0]\n",
      "[0 0 1 0 0 0 0 0 0 0]\n",
      "[1 0 1 0 0 0 0 0 0 0]\n",
      "[0 1 1 0 0 0 0 0 0 0]\n",
      "[1 1 1 0 0 0 0 0 0 0]\n",
      "[0 0 0 1 0 0 0 0 0 0]\n",
      "[1 0 0 1 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    print(binary_encode(i, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6eb58901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fizz_buzz_encode(i):\n",
    "    if   i % 15 == 0: return 3\n",
    "    elif i % 5  == 0: return 2\n",
    "    elif i % 3  == 0: return 1\n",
    "    else: return 0\n",
    "\n",
    "def fizz_buzz(i, prediction):\n",
    "    return [str(i), \"fizz\", \"buzz\", \"fizzbuzz\"][prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7a086774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2415aa5ec10>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import (Dataset, \n",
    "                              DataLoader,\n",
    "                              TensorDataset)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(1)    # reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "950bf83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DIGITS = 10\n",
    "NUM_HIDDEN = 100\n",
    "BATCH_SIZE = 32\n",
    "X = np.array([binary_encode(i, NUM_DIGITS) for i in range(101, 2 ** NUM_DIGITS)])\n",
    "y = np.array([fizz_buzz_encode(i) for i in range(101, 2 ** NUM_DIGITS)])\n",
    "\n",
    "X_train = X[100:]\n",
    "y_train = y[100:]\n",
    "X_valid = X[:100]\n",
    "y_valid = y[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e9e495ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((923, 10), (923,))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b5be7016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([823, 10]),\n",
       " torch.Size([823]),\n",
       " torch.Size([100, 10]),\n",
       " torch.Size([100]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1be6679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_valid = torch.tensor(X_valid, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.int64)\n",
    "y_valid = torch.tensor(y_valid, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "19c41e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FizzBuzz(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.first = nn.Linear(10, 100)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bactnorm1d = nn.BatchNorm1d(100)\n",
    "        self.output = nn.Linear(100, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = self.first(x)\n",
    "        relu = self.relu(a)\n",
    "        batcnorm = self.bactnorm1d(relu)\n",
    "        return self.output(batcnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "54e9f839",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = FizzBuzz()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.03)\n",
    "\n",
    "ds = TensorDataset(X_train, y_train)\n",
    "loader = DataLoader(ds, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "37e39b99",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  tensor(1.2031, grad_fn=<NllLossBackward0>) Training loss:  tensor(1.2707, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(1.1365, grad_fn=<NllLossBackward0>) Training loss:  tensor(1.2441, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(1.1811, grad_fn=<NllLossBackward0>) Training loss:  tensor(1.1383, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(1.2882, grad_fn=<NllLossBackward0>) Training loss:  tensor(1.2332, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(1.3296, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.9795, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(1.3944, grad_fn=<NllLossBackward0>) Training loss:  tensor(1.3072, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(1.4020, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.9569, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(1.3477, grad_fn=<NllLossBackward0>) Training loss:  tensor(1.2010, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(1.3668, grad_fn=<NllLossBackward0>) Training loss:  tensor(1.2139, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(1.3572, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.9919, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(1.3619, grad_fn=<NllLossBackward0>) Training loss:  tensor(1.0801, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(1.3832, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.9620, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(1.3352, grad_fn=<NllLossBackward0>) Training loss:  tensor(1.4019, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(1.3749, grad_fn=<NllLossBackward0>) Training loss:  tensor(1.3649, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(1.3014, grad_fn=<NllLossBackward0>) Training loss:  tensor(1.1571, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(1.2081, grad_fn=<NllLossBackward0>) Training loss:  tensor(1.0001, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(1.1811, grad_fn=<NllLossBackward0>) Training loss:  tensor(1.1890, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(1.2067, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.9872, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(1.2232, grad_fn=<NllLossBackward0>) Training loss:  tensor(1.1736, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(1.0620, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7853, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(1.0007, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8794, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(1.0608, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8112, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(1.1827, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.9301, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.9492, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.9939, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.9022, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8922, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.9519, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6616, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.9128, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8218, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.9481, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7077, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.9140, grad_fn=<NllLossBackward0>) Training loss:  tensor(1.0531, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.7528, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8324, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6883, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6180, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.8670, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.9012, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6398, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6990, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.7471, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6362, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.8297, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8321, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.8161, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6068, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5563, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7044, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.9160, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6458, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6533, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6140, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6319, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8193, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6134, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5513, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5924, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.9372, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5330, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6544, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5136, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3373, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6891, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6547, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6476, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4476, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5392, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7091, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6267, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6485, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4743, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6697, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4596, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4473, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.7371, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3269, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5312, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5256, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.9176, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6408, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.7015, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6442, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.7661, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8137, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.9634, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7392, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.9854, grad_fn=<NllLossBackward0>) Training loss:  tensor(1.1465, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.7666, grad_fn=<NllLossBackward0>) Training loss:  tensor(1.0843, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6933, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7021, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6167, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5580, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6831, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4529, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.7915, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5635, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.7185, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4439, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6671, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8301, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6148, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6961, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6441, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5482, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6766, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4722, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6677, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6440, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.7908, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4851, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  tensor(0.5902, grad_fn=<NllLossBackward0>) Training loss:  tensor(1.2726, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6060, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7573, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6307, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4518, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5956, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8565, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.7028, grad_fn=<NllLossBackward0>) Training loss:  tensor(1.0963, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.7622, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6372, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6502, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4197, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6274, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5813, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.7361, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6477, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.8631, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4055, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6508, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5337, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6925, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8487, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6691, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4575, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.7178, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8818, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5721, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4842, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5939, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5197, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6044, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5249, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6947, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8104, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6964, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4566, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.7102, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7835, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6294, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5166, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6068, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5656, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6051, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6315, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6896, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4436, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6091, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4176, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6190, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6876, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5775, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8598, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6330, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6465, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6966, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7061, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6308, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4296, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6454, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6608, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5852, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7211, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5695, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.9227, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5497, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5574, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6366, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2592, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6413, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3986, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5389, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5118, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5453, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3320, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6619, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6302, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.7370, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3268, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6696, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6513, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5566, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6839, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6007, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4525, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5412, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5727, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5229, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4512, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5952, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.9541, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.7141, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6761, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6240, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5961, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.8338, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3603, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5615, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7818, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.7034, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.9559, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.8707, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4975, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6949, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8673, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.7121, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4941, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6556, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5818, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5760, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5927, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5612, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6779, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5546, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3340, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5636, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8134, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6487, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4973, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5866, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5427, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5978, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7143, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6411, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3177, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5478, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7640, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5644, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6503, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5025, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4737, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5482, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.9597, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6529, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4126, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4786, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3980, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  tensor(0.5144, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2758, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5752, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3011, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6034, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4684, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5779, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.9488, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5031, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4797, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6312, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3144, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4292, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3068, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5148, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3965, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4993, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7143, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5483, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4439, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4619, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4161, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4684, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5943, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4629, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2259, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5553, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2431, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5898, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.9563, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4172, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2878, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4529, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4321, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5271, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5159, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3776, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6263, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4174, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2019, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3444, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3175, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3458, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2400, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3273, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4134, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3341, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4757, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4241, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2249, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4216, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6346, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3557, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3504, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3179, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2961, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2917, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3381, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3103, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2010, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5542, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8008, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2827, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1815, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2814, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6657, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3222, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1760, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3413, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4453, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3947, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0771, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3183, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2529, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3945, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5830, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2902, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5183, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3055, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1824, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2432, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0969, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2343, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7020, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2351, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2638, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3141, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3383, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2493, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8330, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1852, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3761, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2384, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1887, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3424, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3715, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4397, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3941, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2946, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3391, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3431, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2806, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4036, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2783, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2872, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1881, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2348, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2826, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2449, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4717, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2736, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2942, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2829, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1886, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2400, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8478, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2802, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6158, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2013, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1250, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2408, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1581, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3244, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7549, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3237, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5113, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3637, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1263, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4376, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2682, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2727, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2867, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2882, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2959, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2720, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4786, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2848, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2615, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  tensor(0.3216, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4833, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3352, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1601, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3163, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2166, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2541, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2573, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2535, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0512, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4003, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2245, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1906, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4150, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2638, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5953, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5779, grad_fn=<NllLossBackward0>) Training loss:  tensor(1.1195, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2432, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2765, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2560, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2848, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2744, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3045, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2103, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6691, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2655, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3631, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2810, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2830, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2228, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4465, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3102, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4528, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3380, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2809, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2730, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1977, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3008, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5511, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2640, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0558, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3215, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5277, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2875, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2402, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3334, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1800, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1909, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5394, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2804, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7818, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3291, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0710, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3518, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3451, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2324, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3314, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2561, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3739, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3916, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.9667, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4269, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3306, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2919, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1909, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4617, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2020, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2571, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7264, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2240, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3206, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3404, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2894, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3366, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.9356, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2226, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2290, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2441, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5280, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2633, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2501, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2676, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8994, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2695, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1205, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3895, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0600, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2974, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2178, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4930, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3601, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3679, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1572, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2511, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2863, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3615, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3129, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1926, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4620, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2398, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7736, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2715, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0339, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3123, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1076, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2561, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3615, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2637, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1337, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3433, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5296, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2086, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1991, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2272, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2929, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3814, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0535, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2641, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3984, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2475, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1088, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2423, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4403, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1917, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0383, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1976, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7730, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1647, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1550, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2135, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1737, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2766, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2859, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2026, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5220, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2454, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3756, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3444, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6799, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  tensor(0.4302, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1827, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2598, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2739, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4199, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3722, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3591, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4552, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1726, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5096, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4679, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5668, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3529, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3222, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1988, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1811, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2877, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1161, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1721, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2986, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1598, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1404, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2685, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1678, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2337, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4362, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2979, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3615, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3815, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0772, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2919, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4561, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3500, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2247, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4072, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3828, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2030, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2316, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3292, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4334, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2481, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5104, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1873, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2493, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1533, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3997, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2844, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0460, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1460, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0898, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2248, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3051, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1686, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6090, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2387, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1941, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3253, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3485, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3693, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2819, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2712, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4006, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2243, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4800, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2751, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0405, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3057, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7418, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3000, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4497, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2120, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2818, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3713, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2657, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2033, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4034, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2240, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0676, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2962, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2800, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3226, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0896, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1924, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2037, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1992, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1886, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1769, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3949, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3834, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2757, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2365, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1431, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3106, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4022, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2581, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4551, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2735, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4759, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1843, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6505, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3079, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5145, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2472, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2414, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2014, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4867, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2093, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2846, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2236, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0957, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2393, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0224, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2047, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4000, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2292, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0858, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2071, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3393, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2404, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1522, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3988, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5223, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2284, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1576, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1912, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2575, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3095, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1959, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3188, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0612, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2378, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4214, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3117, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5010, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2655, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6660, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2123, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1077, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  tensor(0.3138, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2925, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1810, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2661, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2947, grad_fn=<NllLossBackward0>) Training loss:  tensor(1.0146, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3088, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3777, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2143, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2155, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2350, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4104, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2450, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2468, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3707, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3989, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2694, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2550, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2407, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1131, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2248, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1860, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2430, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3367, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2565, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2249, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2688, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1200, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2170, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1878, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1804, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1909, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1891, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2246, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1768, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2341, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3438, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4291, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4498, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6038, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2617, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4536, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2785, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2475, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3089, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1321, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2568, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3342, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2451, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5936, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2132, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5054, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1691, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5434, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2538, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3840, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1766, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4285, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2003, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2433, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1461, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6138, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2066, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0228, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2813, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1264, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2736, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1427, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2173, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5007, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2726, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3393, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2747, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1833, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3267, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3124, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1470, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4367, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2163, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2872, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2422, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3356, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2066, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6680, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2848, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0843, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2594, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6812, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3309, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2420, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3676, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7857, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2508, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4734, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1384, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2344, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1553, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1894, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2740, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3883, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2304, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0517, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2331, grad_fn=<NllLossBackward0>) Training loss:  tensor(1.1810, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4487, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2179, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2702, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2083, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3379, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1023, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4131, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4281, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1756, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1873, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1665, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5502, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2631, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4234, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2123, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1172, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1505, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1731, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1111, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1382, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1833, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0601, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3460, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0992, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1907, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2647, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3555, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7250, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1773, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4976, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2245, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7400, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1296, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0581, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1929, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1671, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  tensor(0.2099, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2870, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2474, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1482, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2334, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2685, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3748, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1368, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2023, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4349, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1904, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1571, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1951, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1476, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1573, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0912, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1498, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1892, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2105, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4047, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1393, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3567, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2766, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2190, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1654, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6690, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2166, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3306, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2765, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2061, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2032, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1293, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1573, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.9143, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2498, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1026, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2463, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2013, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1634, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5829, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2523, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1535, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3427, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5218, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1681, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5461, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1540, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3646, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2231, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0789, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1822, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4184, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1612, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1988, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1227, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2845, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.0940, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4786, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1700, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3431, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3338, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0484, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1296, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2410, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1893, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3490, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2195, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3170, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2955, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3141, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1809, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0706, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1506, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5085, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1429, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4325, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1910, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8757, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1520, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5003, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2411, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5594, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2205, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4336, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1642, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1970, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3232, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8206, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3396, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1912, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2287, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0694, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1993, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4555, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2232, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6702, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3872, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5552, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1595, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3264, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2286, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2447, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1546, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2331, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2531, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1669, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2512, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5810, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3963, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7048, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3705, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1971, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2939, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1153, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1235, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1762, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1960, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6729, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1378, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3899, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3606, grad_fn=<NllLossBackward0>) Training loss:  tensor(1.3618, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1734, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7840, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1631, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2456, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1498, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0525, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1447, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4051, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1694, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3407, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1999, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3141, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1611, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3048, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1904, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2542, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2490, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3605, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2635, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5515, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1393, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1724, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2152, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6317, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  tensor(0.1316, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1695, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1758, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3288, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2048, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2588, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.0998, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3608, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2208, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0871, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1416, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1079, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1897, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.9394, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2788, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1278, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1715, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6936, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1601, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3670, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2183, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1908, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1461, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3070, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1496, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6397, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1435, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1989, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2531, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3426, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3077, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0513, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1852, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2728, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1321, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2483, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1673, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1005, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1413, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3032, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2221, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1860, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1777, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7697, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2081, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4643, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1826, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3333, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1991, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2699, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1617, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2492, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1625, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6703, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1026, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0691, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1738, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2897, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1332, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3418, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1940, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2662, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2303, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3791, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1900, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6058, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2404, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5349, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1640, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6307, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4211, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8925, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1414, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5451, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2081, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4711, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2117, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3353, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1401, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4866, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1358, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3555, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1165, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5569, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1986, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4507, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3010, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3296, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2726, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6030, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1343, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2780, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2881, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1573, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3530, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2228, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3031, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1764, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1795, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1726, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1639, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7883, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1954, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1561, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2821, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.9915, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2349, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6256, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5597, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2642, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6202, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2526, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2794, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4174, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3126, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0590, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1831, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2851, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4190, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2953, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1747, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1411, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3241, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2115, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2434, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3095, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1647, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5483, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4723, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5832, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1688, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1860, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1894, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2146, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2529, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2328, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1562, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2089, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3581, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3992, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  tensor(0.3311, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1178, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2750, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2375, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1762, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4361, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1442, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3055, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2374, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4373, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1783, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1475, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1888, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0432, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2589, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2573, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2543, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2972, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1471, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0522, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1864, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4916, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3501, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6037, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1827, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8680, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2468, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1495, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1699, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1294, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1030, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2029, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1835, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8355, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1680, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3891, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1682, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2600, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1311, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1247, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1440, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2861, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1842, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1009, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2097, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5511, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1545, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2721, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1691, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1608, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1485, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2942, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3080, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3274, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1606, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5071, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2079, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2056, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1450, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2807, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1057, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2260, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2376, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2737, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2110, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4620, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1387, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5097, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3615, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5870, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1340, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1490, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1997, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2347, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1237, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1486, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1640, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1705, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2031, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1797, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1233, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2153, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1330, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4460, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2921, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1785, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1672, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2038, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1660, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3982, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2487, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2161, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2089, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2691, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1685, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1953, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2765, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1369, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2288, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0791, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1735, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2728, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1957, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3302, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3106, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2997, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4029, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1166, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1575, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6132, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1966, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3742, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2363, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6532, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2239, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1622, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2404, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5182, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1687, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0928, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2069, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2708, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1613, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4201, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2709, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1824, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2378, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5445, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1990, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1868, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2032, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2285, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1474, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0911, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2183, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1734, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1165, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2811, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1710, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0835, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1886, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1639, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  tensor(0.2739, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1083, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2499, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3761, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2659, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3636, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2482, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2505, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2420, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0983, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2484, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5847, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3880, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2772, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2626, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4467, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3112, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0788, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2264, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1548, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3423, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3848, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2091, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1077, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3893, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5511, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1666, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1725, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4370, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1687, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2259, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3968, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2927, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2278, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1824, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2732, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3533, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2198, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4363, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0914, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2855, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4279, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4336, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2655, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2512, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3694, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2106, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2295, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2155, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2201, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2180, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3331, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2214, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6337, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2016, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1535, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2257, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3719, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2037, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3228, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2461, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7319, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2049, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1070, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3440, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2428, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3336, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4558, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2113, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6671, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1617, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2101, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1871, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4457, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1808, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3284, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2105, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2817, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1634, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0349, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2487, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.9587, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1330, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0523, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2028, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7329, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2346, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1749, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1774, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3748, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1654, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4165, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1400, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4179, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4570, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0911, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3982, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3535, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3074, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2736, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2546, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5910, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2250, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5036, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3934, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1665, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2758, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2730, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2980, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0669, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3860, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0202, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2992, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3571, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3850, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6284, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1646, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1772, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3174, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3489, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1573, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3405, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2755, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1871, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2621, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2107, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2159, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3036, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1574, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4788, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2701, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2956, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2844, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0333, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2971, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3808, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2633, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5259, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3011, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1080, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2874, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1681, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  tensor(0.3744, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2086, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4432, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5405, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3445, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4130, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3409, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5960, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1896, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6502, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3264, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2545, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1732, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1638, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3113, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0662, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3836, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1913, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2658, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2812, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3681, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5254, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3156, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0526, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2309, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7277, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3317, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3431, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3238, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2199, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2093, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1765, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2118, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2451, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2823, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2510, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1663, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3404, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2263, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0360, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3195, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5366, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2145, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1492, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4398, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1238, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1434, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5950, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.1449, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4865, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4010, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5619, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2134, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3289, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2832, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0354, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2428, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5284, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4329, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3571, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2743, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0453, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3233, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6268, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3358, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4150, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4056, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3517, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4166, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7912, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2901, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2727, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3553, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1677, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2875, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0598, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2099, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5805, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4641, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1967, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4340, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4456, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4145, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4579, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3294, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1628, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2863, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3480, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5707, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4128, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5020, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4874, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.8250, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0270, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4403, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2004, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4487, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8582, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5545, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1749, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3602, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4501, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4030, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3518, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2617, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3386, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3038, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5001, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3225, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3357, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4810, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3665, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2800, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1737, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3949, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1642, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2400, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4384, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2175, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6481, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2402, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2742, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3499, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0994, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3482, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2662, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3230, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3636, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2325, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3494, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2950, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2689, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3441, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1571, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3750, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3258, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2899, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2052, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  tensor(0.5508, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2538, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2718, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0656, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3854, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6012, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4757, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2536, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3112, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3407, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5048, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4146, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3810, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1265, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3979, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2269, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5829, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5444, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5218, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5928, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4511, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2744, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3570, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3878, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5071, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0881, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2658, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2217, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3759, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1570, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5163, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4818, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4895, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0656, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6024, grad_fn=<NllLossBackward0>) Training loss:  tensor(1.0797, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5593, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0665, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3999, grad_fn=<NllLossBackward0>) Training loss:  tensor(1.4811, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4756, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3584, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3945, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2018, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3908, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1114, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4479, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0729, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3866, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2892, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4329, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4056, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3413, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4600, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4579, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2191, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6842, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3492, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3931, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1293, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4359, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0951, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3650, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0948, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4435, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2359, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3718, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2183, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3709, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0384, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4960, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4763, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4340, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2865, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4377, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1689, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4189, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2116, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5849, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4553, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5615, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2629, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6632, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1974, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4246, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0991, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4861, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2926, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5818, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6810, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5506, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5658, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6229, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0521, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5611, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0437, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4166, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1551, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3679, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0620, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4777, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4100, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3835, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4066, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4117, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0440, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4841, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0229, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4323, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0259, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4864, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0587, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5737, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0504, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5144, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0541, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3678, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4219, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4782, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4391, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4493, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0482, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5251, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0517, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4963, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4246, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5634, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2683, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4230, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0855, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3773, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3543, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4508, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3311, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6039, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0985, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4916, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0550, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  tensor(0.2967, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2520, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3600, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3800, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4414, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0788, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4015, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5728, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2817, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1854, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3493, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2403, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4198, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2624, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4655, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1637, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3205, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0262, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4352, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1080, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4580, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.9912, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3132, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2840, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2749, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1626, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5070, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0692, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.7055, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1826, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3423, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2374, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2886, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6653, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3699, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0580, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3218, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1584, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4540, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3405, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4610, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3314, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4328, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3274, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5706, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5991, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4438, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4174, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4132, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8427, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4670, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1465, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3395, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1096, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4189, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2798, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3854, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1888, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4292, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0263, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3282, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5550, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4422, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2142, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3913, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3960, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.8929, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2166, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4073, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1376, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4875, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2976, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3259, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3192, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4554, grad_fn=<NllLossBackward0>) Training loss:  tensor(1.0703, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6136, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0379, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4902, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5344, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3565, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0856, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4527, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1568, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4022, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2575, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3358, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3777, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4736, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4626, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3839, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0789, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4218, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1855, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3654, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0930, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4144, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0608, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2630, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1787, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4350, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1258, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5663, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6007, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5367, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1033, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3453, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4592, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4889, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1690, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5809, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3351, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3170, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2964, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3848, grad_fn=<NllLossBackward0>) Training loss:  tensor(1.0240, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3340, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4615, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4733, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0422, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3490, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5058, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3340, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.9309, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4952, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4014, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6661, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2791, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3993, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0318, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4457, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.9475, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6447, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2635, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3366, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2676, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4055, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0448, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2932, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6553, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5464, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2550, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  tensor(0.5535, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2975, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3868, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3997, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5936, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0984, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.8789, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4995, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4285, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2273, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4875, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2993, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5277, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3733, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4835, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1685, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3974, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4278, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3887, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0988, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4627, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0304, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4941, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1701, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4096, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2765, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3664, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1474, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3840, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1542, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3748, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0733, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3635, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7211, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5659, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0646, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3645, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1065, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3330, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5209, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4051, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6564, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4526, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5463, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5852, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3552, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5249, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7344, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4051, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1158, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3250, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2737, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5223, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1345, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5427, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2196, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5510, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0907, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5953, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0932, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5780, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2542, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4906, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3910, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3561, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2193, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4435, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3899, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4664, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3682, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3878, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2312, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6051, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1704, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6720, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3100, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4305, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0267, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5093, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0513, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4830, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6349, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3884, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2177, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.7566, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1852, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4944, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3355, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2203, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2489, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4083, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0374, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6170, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1346, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4557, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2272, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4991, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0277, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4143, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3255, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4600, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1128, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5110, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0175, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.7455, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5069, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6526, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1680, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.2501, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7497, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6602, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3969, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5804, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5129, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5246, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4114, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5176, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1239, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5091, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7729, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4807, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1805, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3901, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0470, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5919, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4337, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.7242, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5128, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.6001, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.8364, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.7617, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4776, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4662, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0916, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3611, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0978, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5434, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2107, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  tensor(0.5965, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3652, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4688, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3819, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4364, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4983, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4366, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6128, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.8965, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1465, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4796, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1998, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5335, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.5478, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4044, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1556, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5821, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2478, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4362, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3860, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.3988, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.1587, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5084, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.7315, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5698, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3736, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4866, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.0330, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.7470, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2607, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4926, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.9717, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5080, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.4475, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4553, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.2453, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4570, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6112, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.4270, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.6363, grad_fn=<NllLossBackward0>)\n",
      "Validation loss:  tensor(0.5942, grad_fn=<NllLossBackward0>) Training loss:  tensor(0.3454, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for _ in range(1000):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    net.train()\n",
    "    \n",
    "    for i, (xx, yy) in enumerate(loader):\n",
    "        y_pred = net(xx)\n",
    "        loss = loss_fn(y_pred, yy)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_losses.append(running_loss / i)\n",
    "    \n",
    "    net.eval()\n",
    "    y_pred = net(X_valid)\n",
    "    test_loss = loss_fn(y_pred, y_valid)\n",
    "    print('Validation loss: ', test_loss, 'Training loss: ', loss)\n",
    "    test_losses.append(test_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "de31a2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' 'fizz' '4' 'buzz' 'fizz' '7' '8' 'fizz' 'buzz' '11' '12' '13'\n",
      " '14' 'fizzbuzz' '16' '17' 'fizz' '19' 'buzz' '21' '22' '23' 'fizz'\n",
      " 'fizzbuzz' '26' 'fizz' '28' '29' 'fizzbuzz' '31' '32' 'fizz' '34' 'buzz'\n",
      " 'fizz' '37' '38' 'fizz' 'buzz' '41' '42' '43' '44' 'fizzbuzz' '46' '47'\n",
      " 'fizz' '49' 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59'\n",
      " 'fizzbuzz' '61' '62' 'fizz' '64' 'buzz' 'fizz' '67' '68' 'fizz' 'buzz'\n",
      " '71' 'fizz' '73' '74' 'fizzbuzz' '76' '77' 'fizz' '79' 'buzz' '81' '82'\n",
      " '83' 'fizz' 'buzz' '86' '87' '88' '89' 'fizzbuzz' '91' '92' '93' '94'\n",
      " 'buzz' 'fizz' '97' '98' 'fizz' 'buzz']\n"
     ]
    }
   ],
   "source": [
    "numbers = np.arange(1, 101)\n",
    "X_test = np.array([binary_encode(i, NUM_DIGITS) for i in range(1, 101)])\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "net.eval()\n",
    "_, y_pred = torch.max(net(X_test), 1)\n",
    "\n",
    "output = np.vectorize(fizz_buzz)(numbers, y_pred)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e658fcc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "7268f8b2c8e9b7c101aef22317ecead934d41adaa74e017730d34c40a6ef846d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
